redirect_from: /_posts/2023-09-01-CS224W_NOTES.md
title: Notes for CS224W
tags:

  - 学习笔记

# CS224W Notes

by liyle3



## Lecture 1. Introduction

### 1.1 Why Graphs?

**图是描述和分析具有关系或者相互作用的实体的通用语言**

传统的机器学习：数据样本独立同分布，彼此之间无关系。

图机器学习：数据之间存在连接边的关系。

- 计算机网络
- 疾病传播路径
- 食物网
- 交通路线
- 社交网络
- 网页间的引用
- 神经元关系
- 医疗知识图谱

........



复杂的领域具有丰富的关系结构，可以表示为关系图，通过对关系进行明确地建模，机器学习可以表现出更优秀的性能。



### 1.2 Why is Graph Deep Learning Hard?

<img src="./pics/hard.png" style="zoom:67%;" />







### 1.3 Graph Neural Network

<img src="./pics/GNN.png" style="zoom: 67%;" />

Input：Graph（Network）

Output：

- Node Labels
- New Links
- New Graphs
- New SubGraphs



**Intuition**：节点使用神经网络聚合来自邻居的信息

<img src="./pics/neighbor.png" style="zoom:80%;" />



### 1.4 Representation Learning 

**（监督）机器学习生命周期：**每次都需要选择特征 （特征工程）

<img src="./pics/representation.png" style="zoom:60%;" />



将节点映射到 d 维嵌入，使得网络中相似的节点紧密地嵌入在一起

**特点**

* 端到端的表征学习
* 不需要人工提取特征

<img src="./pics/embedding.png" style="zoom:67%;" />



### 1.5 Applications of Graph ML

**不同任务类型**

* Node level
  * 蛋白质折叠：AlphaFold
* Edge level
  * 推荐系统
  * 药物副作用预测
* Community (subgraph) level
  * 出行路线预测
* Graph level
  * 药物发现
  * 物理仿真

<img src="./pics/tasks.png" style="zoom:67%;" />



* **结点分类**：预测一个结点的特性
* **连接预测**：预测两个结点之间是否存在缺失的连接
* **图分类**：eg. 分子特性预测
* **聚类**：检测结点是否构成一个群体
* 其他
  * 图生成
  * 图演化





### 1.6 Choice of Graph Representation

#### 1.6.1 网络的组成

* 对象：结点，顶点
* 相互作用：连接，边
* 系统：网络、图



**无向图 vs 有向图 (undirected vs directed)**

<img src="./pics/directed.png" style="zoom:80%;" />



#### 1.6.2 异构图

一个异构图可以被定义为：
$$
G = (V, E, R, T)
$$

* 具有结点类型的结点 $v_i \in V$
* 具有关系类型的边 $(v_i, r, v_j) \in E$
* 结点类型 $T(v_i)$
* 关系类型 $r \in R$



#### 1.6.3 结点度数

某个结点的度数 = 与该结点相关联的边的数量

* **无向图**

  平均度数 = $\overline k = <k> = \frac{1}{N} \sum\limits_{i=1}^{N} k_i = \frac{2E}{N}$

* **有向图**

  入度 vs 出度

​		平均度数 （平均入度 & 平均出度）：$\overline k = \frac{E}{N}$

​		$\overline{k^{in}} = \overline{k^{out}}$

​		$source: \ k^{in} = 0, \ sink: k^{out} = 0$ 



#### 1.6.4 二部图

**定义**：图中的结点可以被划分为两个不相交的集合 $U, V$，图中的每一条边都是连接 $U$ 中的一个结点和 $V$ 中的一个结点，即 $U, V$ 是独立的集合

<img src="./pics/二部图.png" style="zoom:67%;" />

#### 1.6.4 邻接矩阵

$N \times N $ 矩阵，矩阵元素均为 0 或 1 （无权图）

<img src="./pics/邻接矩阵.png" style="zoom:67%;" />

**对于无向图，邻接矩阵必为对称矩阵，对于有向图则不一定**

现实应用中的网络大部分为稀疏的图，导致邻接矩阵为稀疏矩阵，即 0 元素占比较大



* **有权图 vs 无权图**

  <img src="./pics/有权图.png" style="zoom:67%;" />

* **自边（自环）& 重边**

  <img src="./pics/自边.png" style="zoom:67%;" />

#### 1.6.5 连通图

* **连通（无向）图**

  无向图中的任意两个顶点均存在一条路径，一个非连通图会由两个或以上的连通分量组成

​		由多个连通分量组成的网络的邻接矩阵可以写成一个块对角矩阵形式

<img src="./pics/块对角.png" style="zoom:67%;" />

* **强连通图**

  有向图中任意两个顶点之间均存在双向路径

* **弱连通图**

  若忽略边的方向（即变成无向图），则该图为连通图

* **强连通分量（Strongly connected components，SCCs）**

  有向图中的子图构成一个强连通图，则为一个强连通分量。

  NOTE：不是每个结点都是非平凡强连通分量的一部分

  <img src="./pics/SCC.png" style="zoom:67%;" />





## Lecture 2. Feature Engineering for ML in Graphs

### 2.1 Traditional ML Pipeline

* 为结点 \ 连接 \ 图设计特征

* 获取所有训练数据的特征

* 训练一个机器学习模型

  * 逻辑回归
  * 随机森林
  * 神经网络
  * ……

* 模型应用

  给出一个新的结点 \ 连接 \ 图，获取新数据的特征并作出预测



### 2.2 Feature Design

* 在图上使用有效的特征 $x$ 是实现良好模型性能的关键
* 传统机器学习管线使用的是手动设计的特征
* 本节内容将对以下几个层面上的传统特征进行概述：
  * Node-level prediction
  * Link-level prediction
  * Graph-level prediction
* 为了简单起见，本节内容关注的是无向图



**Goal**：为一个对象集进行预测

**Design choices**：

* **特征**：$d$ 维向量 $x$
* **对象**：结点、边、结点集合、整个图
* **目标函数**：取决于需要完成的任务



### 2.3 Node-Level Tasks

#### 2.3.1 概述

**Goal**：表征网络中节点的结构和位置

* 结点度数（node degree)
* 结点中心性 （node centrality）
* 聚类系数（clustering coefficient）

* 图基元 （graphlet）



#### 2.3.2 结点度数

* 结点 $v$ 的度数 $k_v$ 是该结点所关联的边的数量（或相邻结点的数量）
* 平等对待所有相邻节点



#### 2.3.3 结点中心性

* 结点度数计算了相邻结点的数量，但是没有考虑它们的重要性
* 结点中心性 $C_v$ 考虑了结点在图中的重要性
* 建模结点重要性的不同方法：
  * 特征向量中心性 (Eigenvector centrality)
  * 中介中心性（Betweenness centrality）
  * 紧密中心性（Closeness centrality）
  * ……



##### 2.3.3.1 特征向量中心性

* 若结点 $v$ 被重要的相邻结点 $u\in N(v)$ 所包围，则结点 $v$ 也是重要的

* 我们将结点 $v$ 的中心性建模为其相邻结点中心性的和
  $$
  c_v = \frac{1}{\lambda} \sum \limits_{u \in N(v)}c_u
  $$
  其中，$\lambda$ 是归一化常数，大小为邻接矩阵 $A$ 最大的特征值

  * 上述计算方法为递归方法，可以重写为以下形式
    $$
    \lambda c = Ac
    $$
    其中，$A$ 是邻接矩阵，$c$ 是中心性向量，$\lambda$ 是特征值

  * 最大的特征值 $\lambda_{max}$ 总是正数且是唯一的，因此选取最大的特征值 $\lambda_{max}$ 对应的特征向量 $c_{max}$ 即为中心性向量



##### 2.3.3.2 中介中心性

* 若一个结点位于多条其他结点之间的最短路径中，则该结点是重要的
  $$
  c_{v}=\sum\limits_{s \neq v \neq t} \frac{\#(\text { shortest paths betwen } s \text { and } t \text { that contain } v \text { ) }}{\#(\text { shortest paths between } s \text { and } t \text { ) }}
  $$
  

<img src="./pics/中介.png" style="zoom:67%;" />





##### 2.3.3.3 紧密中心性

* 若一个结点与其它结点之间有较小的最短路径，则该结点是重要的
  $$
  $c_v=\dfrac{1}{\sum_{u\neq v}\text{shortest path length between } u \text{ and } v}
  $$
  <img src="./pics/紧密.png" style="zoom:67%;" />

##### 2.3.3.4 聚类系数

* 衡量与 $v$ 相邻的节点之间的连通性
  $$
  e_v=\frac{\#(\text{edges among neighbourhood nodes})}{\binom{k_v}{2}}=\frac{2\#(\text{triangles through }v)}{k_v(k_v-1)}\in[0,1]
  $$
  其中，$\binom{k_v}{2}$ 表示 $k_v$ 个相邻结点中的结点对数

  * edge 视角：聚类系数计算了相邻结点之间的边数

    <img src="./pics/系数edge.png" style="zoom:67%;" />

    

  * triangle 视角：聚类系数计算了自我网络（ego-network）中的三角形数量

    <img src="./pics/系数triangle.png" style="zoom:67%;" />

##### 2.3.3.5 图基元 (Graphlet)

**Goal**：描述结点 $u$ 周围的网络结构

图基元是描述结点 $u$ 的网络邻居结构的小子图



**Graphlet Degree Vector**

* 以结点 $u$ 为根结点的不同种图基元的数量构成的向量
* **Graphlets: Rooted connected induced non-isomorphic subgraphs**
  * rooted：以结点 $u$ 为根，如果两个子图形状相同，但是结点 $u$ 在子图中的位置不同，则将其视为两个不同的图基元
  * connected：图基元均为连通图
  * induced：图基元包含原图 $G$ 中的部分结点以及这些结点之间所有的边
  * non-isomorphic：异构



<img src="./pics/graphlet.png" style="zoom:67%;" />



**对比**

* 结点度数计算了结点所关联的边的数量
* 聚类系数计算了结点所关联的三角形
* 图基度向量（Graphlet Degree Vector, GDV）是基于图基元的结点特征



#### 2.3.4 总结

结点特征可以分为以下几类：

* 基于重要性的特征：
  * 结点度数
  * 结点中心性不同的衡量方法
* 基于结构的特征：
  * 结点度数
  * 聚类系数
  * 图基度向量



### 2.4 Link-Level Tasks

**连接预测任务**

* 随机缺失的连接

  随机移除一个连接集合，然后再去预测这些连接

  

* 随时间变化的连接

  * 给出直到时间$t_0'$的边定义的图 $G[t_0, t_0']$，预测在时间段 $G[t_1, t_1']$ 内出现的边
  * 性能评估
    * $n = |E_{new}|$：在测试时间段内出现的新边
    * 取出预测的新边集合中的前$n$个元素，计算正确预测的边数



#### 2.4.1 Link Prediction via Proximity

预测步骤：

* 对于每个结点对$(x, y)$，计算其分数$c(x, y)$
  * eg. $c(x, y)$ 可以是两个结点共同邻居的数量
* 对结点对 $(x, y)$ 按照 $c(x, y)$ 降序排序
* 预测前$n$个结点对作为新的连接
* 检查预测的连接集合中哪些最终会出现在 $G[t_1, t_1']$ 中





#### 2.4.2 Distance-Based Features

* 两个结点之间的最短路径长度
* 缺点：无法衡量两个结点的相邻结点重合度



#### 2.4.3 Local Neighborhood Overlap

表征 $v_1, v_2$ 两个结点之间公共邻居结点的数量

* **Commom neighbors** : 
  $$
  |N(v_1) \cap N(v_2)|
  $$
  
* **Jaccard's coefficient** : 
  $$
  \frac{|N(v_1) \cap N(v_2)|}{|N(v_1) \cup N(v_2)|}
  $$
  
* **Adamic-Adar index** :  
  $$
  \sum_{u\in N(v_1)\cap N(v_2)}\frac{1}{\log(k_u)}
  $$
  其中，$k_u$ 表示结点 $u$ 的度数



<img src="./pics/local1.png" style="zoom:67%;" />

 	

**缺点**：若两个结点没有公共相邻结点，则值为 0，但是这样的两个结点在将来还是有可能会相连

<img src="./pics/local2.png" style="zoom:67%;" />



#### 2.4.4  Global neighborhood overlap

该方法通过考虑整个图来克服上述方法的缺陷

* **Katz index**：计算一对给定结点之间各个长度的路径数量之和
  $$
  S_{v_1, v_2} = \sum \limits _{l=1}^{\infin} A^{l}_{v_1v_2}
  $$
  其中，$0 < \beta < 1$ 为衰减系数，$A^l$ 为邻接矩阵的 $l$ 次幂

  > $P_{v_1v_2}^{(k)}=\mathbf{A}_{v_1v_2}^k$，其中 $P_{v_1v_2}^{(k)}$ 表示 $v_1$ 和 $v_2$ 之间长度为 $k$ 的 walk 的数量

  

* **Katz 指数矩阵可以写成以下闭合形式**
  $$
  S = \sum \limits_{i=1}^{\infin} \beta^iA^i = (I - \beta A)^{-1} - I
  $$

  > $(I - \beta A)^{-1} = \sum \limits_{i=0}^{\infin} \beta ^i A^i$
  >
  > $\beta^0 A^0 = A^0 = I$





### 2.5 Graph-Level Features and Graph Kernels

#### 2.5.1 Kernel Methods

**Idea** ：设计 kernel 来代替特征向量（feature vectors）

* 核 $K(G, G') \in \mathbb{R}$ 衡量了两个图 $G$ 和 $G'$ 的相似度
* 核矩阵 $K = (K(G, G'))_{G, G'}$ 必须是半正定的
* 存在一个特征表示函数 $\phi(\cdot)$ 使得 $K(G, G') = \phi(G) ^T \phi(G')$



**Graph Kernel**

* 目标：设计图特征向量$\phi(G)$
* 总体思想：将词袋模型在图中进行扩展，将结点看作单词，再将结点按照某标准分类（如度数），各类结点数量组成的向量即为 $\phi(G)$



#### 2.5.2 Graphlet Kernel

* 分类标准为图中不同 graphlet 的个数

* 这里的 graphlet 与 node-level feature 中的 graphlet 不同：

  * 不必为连通子图（connected），即允许孤立点的存在
  * 不再选择特定结点作为根结点 （rooted）

  <img src="./pics/graphlet2.png" style="zoom:67%;" />

   

* 对于给定的图 $G$

  * graphlet 的列表为 $G_k = (g_1, g_2, \cdots, g_{n_k})$

  * graphlet 计数向量 $f_G \in \mathbb{R}^{n_k}$ 的定义为：$f(G)_i = \#(g_i \subseteq G) \ \text{for} \ i = 1, 2, \cdots, n_k$

    <img src="./pics/graphlet3.png" style="zoom:67%;" />

    

  * graphlet kernel：
    $$
    K(G, G') = {f_G}^T \ f_{G'}
    $$
    **Problem**：如果两个图具有不同的大小，将会极大影响该值大小

    **Solution**：对每个feature vector进行归一化处理
    $$
    h_G = \frac{f_G}{Sum(f_G)}\\
    K(G, G') = {h_G} ^Th_{G'}
    $$
    



* **缺点**：graphlet 计数的开销极大
  * 在大小为 $n$ 的图中枚举计数大小为 $k$ 的 graphlet 的开销为 $n^k$
  * 子图同构检验 （NP-hard 问题）
  * 若图中结点的度数以 $d$ 为上界，统计大小为 $k$ 的 graphlet 的算法复杂度为 $O(nd^{k-1})$  



#### 2.5.3 Weisfeiler-Lehman Kernel

**Goal**：设计一个高效的图特征描述子 $\phi(G)$

**Idea**：使用邻域结构迭代丰富结点词汇

**算法**：Color refinement



**对于包含结点集合 $V$ 的图 $G$**

* 给每个结点赋予一个初始的颜色 $c^{(0)}(v)$

* 通过以下方法迭代细化结点颜色：
  $$
  c^{(k+1)}(v)=\operatorname{HASH}\left(\left\{c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u\in N(v)}\right\}\right)
  $$
  $HASH$ 函数将不同输入映射成不同的颜色

* 经过 $K$ 步颜色细化后，$c^{(K)}(v)$ 总结了 $K$ 步邻域的结构 

  > 结点度数仅表征了一步邻域的信息

* 最后结果 $\phi(G)$ 为不同颜色的计数向量（颜色在迭代过程中的出现次数也算）

* 优点：计算开销小，时间复杂度与 #(edges) 呈线性关系

  

<center class="half">    
    <img src="./pics/CR1.png" width="500"/>    
    <img src="./pics/CR2.png" width="500"/> 
    <img src="./pics/CR3.png" width="500"/>    
    <img src="./pics/CR4.png" width="500"/> 
</center>





## Lecture 3. Node Embeddings

**Goal**：**任务无关的高效特征学习**



### 3.1 Encoder and Decoder

**Goal**：对结点进行编码，使得两个结点在嵌入空间中的相似度近似于在图上的相似度

<img src="./pics/encoder.png" style="zoom:50%;" />



#### 3.1.1 Encoder-Decoder 框架

<img src="./pics/en-de.png" style="zoom:50%;" />

* 编码器（encoder）：将每个结点映射为一个低维向量
  $$
  ENC(v) = z_v
  $$
  
* 相似度函数（simmilarity function）：衡量结点在图中的相似度，决定了向量空间中的关系如何映射到原始网络中的关系

* 解码器（decoder）：计算两个结点在嵌入空间中的相似度



#### 3.1.2 “Shallow" Encoding

最简单的编码方法：编码器只是进行嵌入查找
$$
ENC(v) = z_v = Z \cdot v
$$

* $Z \in \mathbb R^{d \times |v|}$：一个矩阵，每一列都是一个结点的嵌入向量（需要学习和优化的对象）
* $v \in \mathbb I ^{|v|}$：指示器向量，只有一个位置为 1，其余位置均为 0，用于指示结点 $v$

<img src="./pics/lookup.png" style="zoom:67%;" />



### 3.2 Random Walk Approaches for Node Embeddings

#### 3.2.1 概述

* 无监督/自监督学习，没有利用结点标签或者结点特征
* 这些嵌入是任务无关的，不是为特定任务而学习的，但是可以用于任何任务

* 用从 $u$ 出发的随机游走经过 $v$ 的概率 $P$ 表示节点 $u$ 和 $v$ 的相似度
  * expressivity：捕获局部和高阶邻域信息
  * efficiency：训练时不需要考虑所有的结点对，只需考虑随机游走中出现的结点对

<img src="./pics/rw.png" style="zoom:25%;" />

**步骤**

<img src="./pics/rw步骤.png" style="zoom:40%;" />



对于给定的图 $G(V, E)$，

* 定义 $N_R(u)$ 为从 $u$ 开始的随机游走策略 $R$ 得到的邻域
* 目标是学习到一个由结点到嵌入向量的映射 $f: u\rightarrow\mathbb{R}^d$ ，即 $f(u) = z_u$

* 对数似然目标为：
  $$
  \underset{f}{\max} \sum_{u \in V} \log P(N_R(u) | z_u)
  $$

**算法流程**

* $N_R(u)$ 初始化为空集，从 $u$ 出发进行定长随机游走
* 将经过的节点加入 $N_R(u)$，$N_R(u)$ 为一个多集（multiset），多次经过则多次加入
* 根据以下条件优化嵌入：给定节点 $u$，预测其邻居 $N_R(u)$



最大似然目标为：
$$
\underset{f}{\max} \sum_{u \in V} \log P(N_R(u) | z_u)
$$
等价于
$$
\begin{aligned}\mathcal{L}=\sum_{u\in V}\sum_{v\in N_R(u)}-\log(P(v|\textbf{z}_u))\end{aligned}
$$
其中，$P(v|\textbf{z}_u)$ 由 $\text{softmax}$ 得到（突出相似度最大的结点）：
$$
\begin{aligned}P(v|\mathbf{z}_u)=\frac{\exp(\mathbf{z}_u^{\text{T}}\mathbf{z}_v)}{\sum_{n\in V}\exp(\mathbf{z}_u^{\text{T}}\mathbf{z}_n)}\\ \end{aligned}
$$
<img src="./pics/loss.png" style="zoom:50%;" />



> **直觉**：从 $u$ 出发随机游走，容易遇见的节点 $v$ 在 $N_R(u)$ 中的出现次数就多，权重就大，因此要优先最大化 $P(v|\mathbf{z}_u)$，从而优先最大化两者嵌入向量的相似度，也即内积 $\mathbf{z}_u^{\text{T}}\mathbf{z}_v$



**缺点：**算法开销太大，复杂度为 $O(|V|^2)$

**解决办法：**可以通过负采样优化 $\log (\text{softmax})$
$$
\begin{aligned}\log(\frac{\exp\left(\mathbf{z}_u^{\text{T}}\mathbf{z}_v\right)}{\sum_{n\in V}\exp\left(\mathbf{z}_u^{\text{T}}\mathbf{z}_n\right)})\end{aligned}\approx\log\left(\sigma\left(\textbf{z}_u^\textbf{T}\textbf{z}_v\right)\right)-\sum_{i=1}^k\log\left(\sigma\left(\textbf{z}_u^\textbf{T}\textbf{z}_{n_i}\right)\right),n_i\sim P_V
$$
其中， $\sigma (x)=\frac{1}{1+\exp(-x)}$ 为 $\text{sigmoid}$ 函数

* 此时只需要按照概率采样 $k$ 个负节点 $n_i$，概率与结点度数成正比

* k 的取值通常为 5-20
  * k 越大，估算结果越可靠
  * k 越大，对负面事件的偏差越大



**优化方法：随机梯度下降（SGD）**

1. 对所有节点 $u$，将 $\mathbf{z}_u$ 初始化为随机值
2. 不断迭代直到收敛：$\mathcal{L}^{(u)}=\sum_{v\in N_R(u)}-\log(P(v|\mathbf{z}_u))$
   1. 随机采样一点 $u$，对所有节点 $v$ 计算导数 $\frac{\partial L^{(u)}}{\partial z_v}$。
   2. 对所有 $v$，更新 $z_v\leftarrow z_v-\eta\frac{\partial{\mathcal{L}^{(u)}}}{\partial z_v}$



#### 3.2.2 游走策略

* 最简单的游走策略显然是定长无偏随机游走：DeepWalk
* 也可以使用有偏随机游走算法以权衡局部和全局信息：node2vec

[DeepWalk & node2vec](https://www.cnblogs.com/dogecheng/p/13198198.html)



**node2vec**：

<img src="./pics/node2vec.png" style="zoom:50%;" />

相对而言，BFS 捕获局部信息，DFS 捕获全局信息

定义两个参数

* 返回参数 $p$：返回上一节点
* 入出参数 $q$：BFS vs. DFS 的比例

<img src="./pics/node2vec2.png" style="zoom:50%;" />



**算法：**

<img src="./pics/node2vec3.png" style="zoom:50%;" />



### 3.3 Embedding Entire Graphs

**Goal：**将一个子图或者整个图映射为一个嵌入向量 $\mathbf z_G$

* 方法一（简单却有效）：将所有结点的嵌入向量求和取平均，即 $\mathbf{z}_G=\sum \limits _{v\in G}\mathbf{z}_v$

* 方法二：创建一个虚拟结点，用于表示（子）图，计算虚拟结点的嵌入向量



**层次化嵌入：**先聚类，再求平均

<img src="./pics/cluster.png" style="zoom:67%;" />





### 3.4 Matrix Factorization and Node Embeddings

若定义图中节点相似度为有边连接为 1，无边连接为 0，则 $\mathbf{Z}^\text{T}\mathbf{Z}\approx\mathbf{A}$ 即要求邻接矩阵的分解

<img src="./pics/分解.png" style="zoom:60%;" />

DeepWalk 也可转为矩阵分解：

<img src="./pics/deepwalk.png" style="zoom:60%;" />



**通过矩阵分解和随机游走算法来实现 node embeddings 的缺陷**：

* 无法获得不在训练集的结点的嵌入，加入新结点需要重新计算所有的嵌入
* 无法捕获结构相似性
* 无法利用其它点、边、图的特征



### 3.5 PageRank

#### 3.5.1 The "Flow" Model

$$
r_j = \sum \limits _{i \rightarrow j} \frac{r_i}{d_i}
$$

* $r_i$ 是结点 $i$ 的 rank
* $d_i$ 是结点 $i$ 的度数

<img src="./pics/flowmodel.png" style="zoom:50%;" />

##### 3.5.1.1 矩阵形式

* 随机邻接矩阵（Stochastic Adjacency Matrix）$M$
  * 假设页面 $j$ 有 $d_j$ 个外链（out-link）
  * 如果有 $j \rightarrow i$，则 $M_{ij} = \frac{1}{d_j}$
    * $M$ 是一个列随机矩阵（column stochastic matrix）：每一列元素相加之和均为 1

<img src="./pics/列随机.png" style="zoom:67%;" />

* 秩向量（Rank Vector）$r$

  * $r_i$ 是页面 $i$ 的重要度分数
  * $\sum_i r_i = 1$

  

* The flow equation：
  $$
  r = M \cdot r
  $$
  

##### 3.5.1.2 与随机游走的联系（random web surf）

* $p(t)$：一个向量，用于表示在时刻 $t$ 时页面之间的概率分布，第 $i$ 维表示时刻 $t$ 浏览者停留在页面 $i$ 上的概率

* 迭代形式：$p(t+1) = M \cdot p(t)$

* 当随机游走到达以下状态时：
  $$
  p(t+1) = M \cdot p(t) = p(t)
  $$
  则称 $p(t)$ 是随机游走的稳态分布（stationary distribution）

$$
r = M \cdot r \\ p(t) = M \cdot p(t)
$$

##### 3.5.1.3 特征向量形式

* **The flow equation**
  $$
  1 \cdot r = M \cdot r
  $$
  可以认为 $r$ 是矩阵 $M$ 特征值 1 对应的特征向量

  * 假设从向量 $u$ 开始迭代，则极限 $M(M(\dots M(Mu)))$ 称为浏览者的长期分布
  * PageRank = 极限分布 = 特征向量 



##### 3.5.1.4 迭代求解过程

<img src="./pics/iteration.png" style="zoom:50%;" />

##### 3.5.1.5 Problems

* **dead ends**

  <img src="./pics/deadend.png" style="zoom:50%;" />

  **会导致重要度"泄露"**

  

* **spider traps**

  <img src="./pics/spidertrap.png" style="zoom:50%;" />

  **最终蜘蛛陷阱会吸收掉所有的重要度**

<img src="./pics/sol.png" style="zoom:60%;" />



#### 3.5.2 Random Teleports

* 随机浏览者在每一步都有两个选择：

  * 有 $\beta$ 的概率随机选择一个链接

  * 有 $1-\beta$ 的概率跳转到一些随机页面中

    

* **PageRank equation**
  $$
  r_j = \sum \limits _{i \rightarrow j} \beta \frac{r_i}{d_i} + (1-\beta)\frac{1}{N}
  $$
  上述等式假设 $M$ 中不存在死路（dead ends），或者可以对矩阵 $M$ 进行预处理，去除所有的死路或将随机传送的概率设置为 1。

  

* **The Google Matrix G**
  $$
  G = \beta M + (1-\beta) \left[ \frac{1}{N} \right]_{N \times N}
  $$

  * 由此同样可以得到一个递归问题：$r = G \cdot r$

  * 实践中，$\beta$ 的取值一般为0.8 or 0.9



#### 3.5.3 Personlized PageRank

* 可以传送到指定的结点集合 S

* 集合 S 中的结点传送到达的概率可以不相同

  $S = [0.1, 0, 0, 0.2, 0, 0, 0.5, 0, 0, 0.2]$

​	**可以用于对结点与传送结点集合 S 的接近程度进行排名**



#### 3.5.4 Random Walks with Restarts

* 与 **Personlized PageRank** 类似，但是总是传送到相同的结点

  $S = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]$



## Lecture 4. GNN

### 4.1 Basics of deep learning

#### 4.1.1 优化问题

目标函数为：
$$
\min_{\Theta} L(y, f(x))
$$

* $\Theta$：需要优化的一系列参数

* $L$：损失函数

  * 如 L2 loss ：
    $$
    L(y, f(x)) = ||y -f(x)||_2
    $$
    

#### 4.1.2 多层感知机（Multi-layer Percepton）

感知机的每一层都结合了线性变换和非线性处理
$$
x^{(l+1)} = \sigma(W_lx^{(l)} + b^l) 
$$

* $W_l$ 是权重矩阵，用于将隐藏表达从第 $l$ 层转换到第 $l + 1$ 层
* $b^l$ 是第 $l$ 层的偏差，将会被添加至 $x^{(l)}$ 的线性变换中
* $\sigma$ 是非线性函数（如 sigmoid 函数） 



### 4.2 Deep Learning for Graphs

#### 4.2.1 A Naïve Approach

* 将邻接矩阵和特征拼接在一起
* 将拼接后的矩阵送入（全连接） 深度神经网络中

<img src="./pics/naive.png" style="zoom:80%;" />

* 存在的问题
  * 参数规模为 $O(|V|)$
  * 不适用于不同大小的矩阵
  * 对结点序号敏感



#### 4.2.2 Convolutional Networks

$A \in \mathbb {R}^{|V| \times |V|}$ 为邻接矩阵，$X \in \mathbb{R} ^{|V| \times d}$ 为结点特征矩阵，则图 $G = (A, X)$

> 与图片不同，图没有标准的结点顺序，因此 表征（representations）应该保证不随着结点顺序变化而变化，否则同一个图中的结点顺序变化之后，训练好的模型就不再适用



**排列不变性（permutation invariance）**

> 函数 $f$ 将图 $G = (A, X)$ 映射为嵌入向量 $f(G) \in \mathbb{R}^d$
>
> 若 $f(A, X) = f(PAP^T, PX)$，其中 $P$ 为排列矩阵
>
> 则函数 $f$ 具有排列不变性



**排列同变性（permutation equivariance）**

> 函数 $f$ 将图 $G$ 中的所有节点 $u$ 映射为 嵌入向量 $\mathbf{z}_u \in \mathbb{R}^d$，即将图 $G$ 映射为矩阵 $\mathbb{R}^{m \times d}$
>
> 若 $Pf(A, X) = f(PAP^T， PX)$，其中 $P$ 为排列矩阵
>
> 则函数 $f$ 具有排列同变性



排列不变性：对于同一个图 $G$，在不同结点顺序下的表征相同

排列同变性：对于同一个结点，在不同结点顺序下的表征相同

排列不变性意味着函数不依赖于邻接矩阵中行/列的排列顺序；排列同变性则意味着当我们对邻接矩阵进行置换时，函数 $f$ 的输出以相同的方式被置换



**GNN 包含多个满足排列不变性/排列等变性的函数**

<img src="./pics/permutation.png" style="zoom:67%;" />	



### 4.3 Graph Convolutional Networks

#### **4.3.1 核心思想：**从相邻结点中聚合信息，基于本地网络邻域生成结点嵌入

#### **4.3.2 计算图（computation graph）**

<img src="./pics/computation.png" style="zoom:67%;" />



#### **4.3.3 Deep Encoder**

**基本方法：**将邻居信息取平均，并送入神经网络

<img src="./pics/average.png" style="zoom:67%;" />

**基本算法**：

1. 初始化：$h_{v}^{0}=x_{v}$
2. 迭代：$h^{(k+1)}_v=\sigma(W_k\sum_{u\in N(v)}\dfrac{h^{(k)}}{|N(v)|}+B_kh^{(k)}_v),\forall k\in\{0,\ldots,\mathbb{N}-1\}$，其中 $\sigma$ 为激活函数，$K$ 为迭代次数，$W_k$ 为可学习参数
3. 输出：$\mathbf{z}_v=h_v^{(K)}$

> 节点 $u$ 在第 $k$ 层的编码等于参数矩阵 $W_k$ 乘以它的相邻节点在 $k-1$ 层编码的平均，再加上参数矩阵 $B_k$ 乘以它本身在 $k-1$ 层的编码，最后通过激活函数，该方法与顺序无关，因此满足排列不变性/排列同变性

我们可以将这些嵌入向量送入任意损失函数 并运行随机梯度下降法（SGD）来训练权重参数



**算法矩阵化**

1. 令 $H^{(k)}=[h_{1}^{(k)}...h_{|V|}^{(k)}]^{\text{T}}$
2. 注意到 $\sum_{u\in N_v}h_{u}^{(k)}=\mathbf{A}_{v}\mathop{\text{H}}^{(k)}$，其中 $A_v$ 表示邻接矩阵 $A$ 中结点 $v$ 对应的行向量
3. 令 $D_{v,v}=\text{Deg}(v)=|N(v)|$
4. 则 $H^{(k+1)}=D^{-1}\mathbf{A}H^{(k)}$，即 $H^{(k+1)}=\sigma(\tilde{A}H^{(k)}W_k^{\text{T}}+H^{(k)}B_k^{\text{T}})$，其中 $\tilde{A}=D^{-1}A$ 为一个稀疏矩阵

<img src="./pics/矩阵化.png" style="zoom:67%;" />

#### **4.4.4 模型训练**

* **监督学习**：最小化损失函数
  $$
  \min_{\Theta} L(y, f(x))
  $$

* **无监督学习**：

  * 无结点标签

  * 需要利用图本身的结构来构造损失函数
    $$
    \mathcal{L}=\sum_{z_u,z_v}\mathrm{CE}(y_{u,v},\mathrm{DC}(z_u,z_v))
    $$

    * 当结点 $u$ 和结点 $v$ 相似时， $y_{u, v} = 1$ 
    * $\mathrm{CE}$ 为交叉熵
    * $\mathrm{DEC}$ 是解码器（如向量内积）



#### 4.4.5 优点

* 所有结点共享所有的聚合参数，模型参数的数量与结点数量 $|V|$ 为次线性关系
* 可以推广到新的图
* 可以高效地为新到来的结点生成嵌入向量



### 4.4 GNNs subsume CNNs

<img src="./pics/GNNvsCNN.png" style="zoom:67%;" />

CNN 可被视为相邻点的尺寸与顺序固定的特殊 GNN，它可以为节点周围与本身共 9 个位置都赋予一个共享参数，因此不满足排列不变性/排列同变性

**Key difference**：CNN 可以为图像中像素点 $v$ 的不同邻居学习不同的权重参数 $W_l^u$（原因在于可以使用与中心像素的相对位置为 9 个邻居选择一个顺序）



## Lecture 5. A General Perspective on Graph Neural Networks

### 5.1 A Single Layer of a GNN

GNN层：

* 将一组向量压缩为一个向量
* 两步过程
  * 消息（Message）
  * 聚合（Aggregation）

<center class="half">    
    <img src="./pics/layer.png" width="400"/>    
    <img src="./pics/compress.png" width="400"/> 
</center>



#### 5.1.1 Message

* 每个节点生成一个消息传递给下一层的节点
  $$
  \textbf{m}_u^{(l)}=\text{MSG}^{(l)}\left(\textbf{h}_u^{(l-1)}\right)
  $$
  其中 $\text{MSG}$ 为消息函数

  > 对于线性层，消息函数为 $\text{MSG}^{(l)}(\textbf{h}_u^{(l-1)})=\mathbf{W}^{(l)}\textbf{h}_u^{(l-1)}$（节点特征与权重矩阵相乘）



* 每个节点聚合其相邻节点传来的消息
  $$
  \mathbf{h}_{v}^{(l)}=\text{AGG}^{(l)}\left(\left\{\mathbf{m}_{u}^{(l)},u\in N(v)\right\}\right)
  $$
  其中$\text{AGG}$ 为聚合函数

  > 聚合函数可以是求和，求平均，求最大值

  但此时节点 $v$ 本身的信息在传递给下一层时可能会丢失，为了保证自身消息的传递，传递形式修改为：
  $$
  \mathbf{h}_v^{(l)}=\text{CONCAT}\left(\text{AGG}\left(\left\{\mathbf{m}_u^{(l)},u\in N(v)\right)\right);\mathbf{m}_v^{(l)}\right)
  $$
  其中，$\textbf{m}_v^{(l)}=\textbf{B}^{(l)}\textbf{h}_v^{(l-1)}$，$\text{CONCAT}$ 为拼接函数



**Classical GNN Layers**

* **Graph Convolutional Networks (GCN)**
  $$
  \textbf{h}_v^{(l)} = \sigma (\sum_{u \in N(v)} W^{(l)} \frac{h_u^{(l-1)}}{|N(v)|})
  $$
  

  * **消息：**$\textbf{m}_u^{(l)}=\frac{1}{|N(v)|}\textbf{W}^{(l)}\textbf{h}_u^{(l-1)}$

  * **聚合：**$\mathbf{h}_{v}^{(l)}=\sigma\left(\text{Sum}\left(\left\{\mathbf{m}_{u}^{(l)},u\in N(v)\right\}\right)\right)$

    

* **GraphSAGE**
  $$
  \mathbf{h}_{v}^{(l)}=\sigma\left(\mathbf{W}^{(l)}\cdot\text{CONCAT}\left(\mathbf{h}_{v}^{(l-1)},\text{AGG}\left(\left\{\mathbf{h}_{u}^{(l-1)},\forall u\in N(v)\right\}\right)\right)\right)
  $$

  * 消息在 $\text{AGG}(\cdot)$ 函数中计算

  * 聚合分两步：先聚合相邻节点，再和节点 $v$ 本身拼接起来

  * 聚合函数 $\text{AGG}$ 取值

    * 平均：
      $$
      \text{AGG}=\sum\limits_{u\in N(v)}\frac{\mathbf h_u^{(l-1)}}{|N(v)|}
      $$

    * 池化：
      $$
      \text{AGG}=\text{Mean}(\{\text{MLP}(\textbf{h}^{(l-1)}u),\forall u\in N(v)\})
      $$

    * LSTM：
      $$
      \text{AGG}=\text{LSTM}([\mathbf{h}_u^{(l-1)},\forall u\in\pi\big(N(v)\big)])
      $$
      

  * $l_{2}$ 正则化：在每一层的 $h_v^{(l)}$ 中加入正则化项
    $$
    \textbf{h}_v^{(l)}\leftarrow\frac{\textbf{h}_v^{(l)}}{\left\|\textbf{h}_v^{(l)}\right\|_2}\quad\forall v\in V\
    $$
    其中 $||u||_2 = \sqrt{\sum_i u_i^2}$

    * 保证嵌入向量（二范数）数量级相同
    * 在某些情况下，$l_2$ 正则化可以提升性能
    * 正则化后所有向量拥有相同的二范数



* **Graph Attention Networks (GAT)** 
  $$
  \textbf{h}_v^{(l)}=\sigma(\sum_{u\in N(v)}\alpha_{vu}\textbf{W}^{(l)}\textbf{h}_u^{(l-1)})
  $$

  * 在 GNN 和 GraphSAGE 里
    * 节点 $u$ 的消息到达节点 $v$ 的权重因子（重要性）$\alpha_{vu}=\frac{1}{|N(v)|}$
    * $\alpha_{uv}$ 是基于图的结构特性（节点度数）明确定义的
    * 节点 $v$ 的所有相邻节点 $u \in N(v)$ 具有相同的重要性
    * 但实际上不同的相邻节点其实重要性不同，我们希望这一权重可以被学习

  

  **Goal：**为同一个节点的不同邻居指定任意的重要度（而不是相同的重要度）

  **Idea：**在为每个节点计算嵌入向量时遵循以下的注意力策略

  * 节点参与到其相邻节点的消息中
  * 为邻域中的不同节点隐式地指定不同的权重

  

  **Attention Mechanism**

  * 注意力系数的定义如下：
    $$
    e_{vu}=a(\textbf{W}^{(l)}\textbf{h}_u^{(l-1)},\textbf{W}^{(l)}\textbf{h}^{(l-1)}_v)
    $$
    用于表征来自 $u$ 的消息对 $v$ 的重要性，其中 $a$ 为注意力机制函数

  * 将 $e_{uv}$ 归一化为最终的注意力权重 $\alpha_{uv}$（使用 $\text{softmax}$ 函数，以使得 $\sum_{u\in N(v)} \alpha_{uv} = 1$）
    $$
    \alpha_{vu}=\dfrac{\text{exp}(e_{vu})}{\sum_{k\in N(v)}\text{exp}(e_{vk})}
    $$
    
  * 基于最终注意力权重 $\alpha_{uv}$ 的带权重和为：
    $$
    \textbf{h}_{v}^{(l)} = \sigma(\sum_{u\in N(v)} \alpha_{uv} \textbf{W}^{(l)} \textbf{h}_u^{(l-1)})
    $$
  
  
  
  **注意力机制介绍**
  
  * 一个简单的注意力机制实例： $a(A,B)=\text{Linear}(\text{Concat}(A,B))$ ，其中 Concat 就是简单的向量拼接，Linear 的参数是可学习的
  
    <img src="./pics/attention1.png" style="zoom:70%;" />
  
    
  
  * 多头注意力机制 
  
    * $\textbf{h}^{(l)}_v[i]=\sigma (\sum_{u\in N (v)}\alpha^{(i)}_{vu}\textbf{W}^{(l)}\textbf{h}^{(l-1)}_u)\quad i=1,2,...,n_a$
    * $\textbf{h}_v^{(l)}=\text{AGG}(\textbf{h}_v^{(l)}[1],\textbf{h}_v^{(l)}[2],...,\textbf{h}_v^{(l)}[n_a])$
  
    <img src="./pics/attention2.png" style="zoom:67%;" />
  
  **注意力机制的优点**
  
  * **key benefit**：允许为不同的相邻节点指定不同的重要度（权重）
  
  * 可并行计算
  * 所需储存空间与图大小无关
  * 只注意局部邻域
  * 共享的沿边机制，不依赖全局结构



### 5.2 GNN Layers in Practice

一些经典的神经网络技巧在 GNN 里也适用：

* **Batch Normalization**

  * **Goal：**使神经网络训练变得稳定
  * **Idea：**
    * 中心化嵌入向量以使得均值为 0
    * 将方差调整为单位方差（即方差为1）

  <img src="./pics/bn.png" style="zoom:67%;" />

* **Dropout**

  * **Goal：**规范化神经网络以防止过度拟合 

  * **Idea：**

    * training：以一定的概率 $p$ 随机将神经元置为 0
    * testing：将所有的神经元均用于计算

  * 主要应用在消息函数的线性层。

    <img src="./pics/dropout.png" style="zoom:67%;" />

  

* **Activation**：ReLU、Sigmoid、Parametric ReLU

<img src="./pics/activation.png" style="zoom:67%;" />



### 5.3 Stacking Layers of a GNN

连接 GNN 层的方式：

* 按顺序堆叠层（构建 GNN 的标准方式）
* 添加跳过连接的方法



**过平滑问题**：所有节点的嵌入向量收敛到同一个值

**接收域**：决定节点 $v$ 的所有节点集合

* 在 $k$ 层 GNN 中，每个节点都有一个 $k$ 跳邻域的接收域
* 当跳数（GNN 层数）增加时，共享的相邻节点数量会快速增加



GNN 堆叠层数过多 $\Rightarrow$ 节点拥有高度重叠的接收域 $\Rightarrow$ 节点的嵌入向量高度相似 $\Rightarrow$ 过平滑问题



**解决过平滑问题的方法：**

* Lesson 1：谨慎添加 GNN 层

  * step 1：分析必要的接收域

  * step 2：设置 GNN 层数 $L$ 使得实际接收域仅略大于必要的接收域（$L$ 不能设置地过分大）

    

  **Problem：**但若 GNN 层数较少，表达能力会降低

  * Solution 1：加强每个 GNN 层的表达能力。比如把 3 层 MLP 作为消息/聚合函数

    <img src="./pics/express1.png" style="zoom:50%;" />

  * Solution 2：添加不传递消息的层，比如在 GNN 层的前后添加 MLP。

    * 预处理：需要编码节点特征时比较重要
    * 后处理：需要节点上的推理/转换时比较重要

    <img src="./pics/express2.png" style="zoom:67%;" />

* Lesson 2： 为 GNN 添加跳过连接（skip connection）

  * GNN 的前几层可能能更好地区分节点，因此需要更多地保留（通过在 GNN 中加入 shortcuts）
  * 每一层分跳过和不跳过，会得到 $2^N$ 种可能的路径，每条路径最多有 $N$ 个模块
  * Example
    * 标准 GCN 层：$$\textbf{h}_{v}^{(l)}=\sigma\left(\sum_{u\in N(v)}W^{(l)}\frac{\textbf{h}_{u}^{(l-1)}}{|N(v)|}\right)$$    $\sigma\left(F(x)\right)$
    * 添加了跳过连接的 GCN 层：$\textbf{h}_{v}^{(l)}=\sigma\left(\sum_{u\in N(v)}W^{(l)}\frac{\textbf{h}_{u}^{(l-1)}}{|N(v)|}+\textbf{h}_{v}^{(l-1)}\right)$   $\sigma\left(F(x) + x\right)$
  * 甚至可以设置跳过多个层

  <img src="./pics/lesson2.png" style="zoom:80%;" />



### 5.4 Graph Manipulation in GNNs

目前为止，我们都将原始输入的图作为计算图，但实际上这常常并不是最优解。原始的图可能 

1. 缺乏特征；

2. 图太稀疏：消息传递效率低下；

3.  图太稠密：消息传递开销太大；

4. 图太大：无法将计算图装入 GPU

   

解决方法：

* Solution 1：特征增强（feature augmentation）

  * Problem 1：输入图可能没有节点特征，即只有邻接矩阵

    * Approach 1：为节点赋予常量
      * 中表达能力、强归纳能力（即预测没见过的节点的能力）、低计算开支、适用任何图
    * Approach 2：为节点赋予唯一 ID，并转为独热（one-hot）编码
      * 高表达能力、无归纳能力、高计算开支、只适用于小图

    ![](./pics/one-hot.png)

    

  * Problem 2：有些特定结构难以被 GNN 学习

    * 如 GNN 无法学到节点 $v$ 所在环的长度，因为计算图是同一个棵二叉树。此时可以使用独热编码，其中对应所在环长度的位置为 1，其他为 0。
    * L2 中 Node-Level Tasks and Features 中所述的 feature 都可以被用作特征增强

* Solution 2：结构增强：添加虚拟边

  * 用虚拟边连接 2 跳相邻节点，即用 $\mathbf{A}+\mathbf{A}^2$ 来计算 GNN。
  * 适用于**二分图**，如作者-论文图中 2 跳虚拟边可以将合作的作者相连。

* Solution 3：结构增强：添加虚拟节点

  * 添加一个和所有节点相连的虚拟节点。
  * 适用于**稀疏图**，原本离得很远的节点之间距离直接缩短为 2，可以大幅提高信息传递。

* Solution 4：节点邻域采样

  * 不从邻域内的所有节点中获取消息，而是从中随机采样。
  * 适用于**稠密图**，大幅降低计算开销，而且最终得到的嵌入向量不会和计算邻域内的全部节点的结果相差很多。



## Lecture 6. GNN Augmentation and Training

### 6.1 Prediction with GNNs

GNN 的输出是一系列节点的嵌入向量，即 $\{\textbf{h}_v^{(L)},\forall v\in G\}$，我们通过预测头（prediction head）把它转化为预测。设所需预测为 $k$ -way 的，即输出为一个 $k$ 维向量。

* 节点级预测
  * $\widehat{\mathbf{y}}_{\boldsymbol{v}}=\text{Head}_{\text{node}}(\mathbf{h}_{\boldsymbol{v}}^{(L)})=\mathbf{W}^{(H)}\mathbf{h}_{\boldsymbol{v}}^{(L)}$，其中 $\textbf{W}^{(H)}\in\mathbb{R}^{k\times d}$
  * 即直接乘以一可学习的参数矩阵 $\textbf{W}^{(H)}$
* 边级预测
  * $\widehat{\mathbf{y}}_{uv}=\text{Head}_{\text{edge}}(\mathbf{h}_{u}^{(L)},\mathbf{h}_{v}^{(L)})$
  * $\text{Head}_{\text{edge}}(\mathbf{h}_{u}^{(L)},\mathbf{h}_{v}^{(L)})$ 的选择
    * 拼接+线性层： $\widehat{\mathbf{y}}_{uv}=\text{LINEAR}(\text{CONCAT}(\mathbf{h}_u^{(L)},\mathbf{h}_v^{(L)}))$
    * 内积
      * $k=1$：$\widehat{y}_{uv}=(\textbf{h}_{u}^{(L)})^T\textbf{h}_{v}^{(L)}$
      * $k>1$：$\widehat y_{uv}^{(i)}=(\textbf h_u^{(L)})^T\textbf W^{(i)}\textbf h_v^{(L)}$，$\widehat{y}_{uv}=\text{CONCAT}(\widehat{y}_{uv}^{(1)},...,\widehat{y}_{uv}^{(k)})\in\mathbb{R}^k$
* 图级预测
  * $\widehat{\mathbf{y}}_G=\text{Head}_{\text{graph}}(\{\mathbf{h}_v^{(L)}\in\mathbb{R}^d,\forall v\in G\})$
  * $\text{Head}_{\text{graph}}(\{\mathbf{h}_v^{(L)}\in\mathbb{R}^d,\forall v\in G\})$ 的选择
    * 整体平均、最大值、求和池化。只适用于小图。
    * 分层聚合：先对节点根据其嵌入向量聚类，每一簇聚合为一节点，反复进行，直到只剩一个节点，以此作为图的输出
    * ![](D:\DESKTOP\ML\ML_Graph\reference\CS224W-Winter2023\notes\assets\Pasted image 20230302174119.png)



### 6.2 Training Graph Neural Networks

训练用的 Ground-truth （即 $y$ ）的来源

* 有监督标签：外部信息，包括分子类型等；
* 无监督（自监督）信号：clustering coefficient, PageRank 等
  它们又各自分为节点级 $y_v$、边级 $y_{uv}$ 和图级 $y_G$。

损失函数

* 分类任务：交叉熵（CE）
  $$
  \text{CE}\left (\boldsymbol{y}^{(i)},\boldsymbol{\widehat{y}^{(i)}}\right)= \sum_{j=1}^{K}\boldsymbol{y}_{j}^{(i)}\log\left (\boldsymbol{\widehat{y}_j^{(i)}}\right)
  $$

* 回归任务：均方误差（MSE）
  $$
  \text{MSE}\bigl(\boldsymbol{y}^{(i)},\boldsymbol{\widehat{y}}^{(i)}\bigr)=\sum_{j=1}^{K}(\boldsymbol{y}_j^{(i)}-\boldsymbol{\widehat{y}}_j^{(i)})^2
  $$
  

评价指标

* 回归任务：
  * 均方根误差均方根误差（RMSE）：$\sqrt{\sum_{i=1}^N\frac{(\boldsymbol{y}^{(i)}-\boldsymbol{\widehat y}^{(i)})^2}{N}}$
  * 平均绝对值误差（MAE）： $\left.\frac{\sum_{i=1}^N\left|\boldsymbol{y}^{(i)}-\boldsymbol{\widehat{y}^{(i)}}\right|}{N}\right.$
* 分类任务
  * Multi-class 分类（多选一）：$\frac{\mathbb{I}\left[\text{argmax}\left(\widehat{\boldsymbol{y}}^{(i)}\right)=\boldsymbol{y}^{(i)}\right]}{N}$
  * 二分类：Accuracy，Precision，Recall，F1-Score，ROC，AUC

### 6.3 Setting-up GNN Prediction Tasks

数据集划分

* 训练集：最优化 GNN 参数
* 验证集：调整超参数
* 测试集：检验效果
  对不同的随机数种子进行随机划分

节点互相连接，互相影响嵌入向量，训练集中的节点可能和测试集相连

* Solution 1：传导设置（Transductive setting）

  * 所有划分部分都能看到整个图结构，只对节点标签进行划分
  * 不能泛化到没见过的图，只适用于节点/边预测任务

  <img src="./pics/split1.png" style="zoom:67%;" />

* Solution 2：归纳设置（Inductive setting）

  * 破坏不同划分部分之间的边，使输入图变成多个图
  * 能泛化到没见过的图，适用于节点/边/图预测任务

  <img src="./pics/split2.png" style="zoom:67%;" />

例子：连接预测

* Option 1：归纳设置

  * 把边随机分为 2 类，即消息边和监督边
    * 消息边：将会保留，用于在 GNN 中传递消息
    * 监督边：将被从图中消除，用于计算目标函数
  * 数据集包含多个图，每个图里的边都有消息边和监督边两种。训练/验证/测试时都只使用自己的节点，用消息边去预测监督边

  <center class="half">    
      <img src="./pics/op1-1.png" width="400"/>    
      <img src="./pics/op1-2.png" width="400"/> 
  </center>

* Option 2：传导设置（常用）

  * 把边随机分为 4 类，即 a. 训练消息边、b. 训练监督边、c. 验证边、d. 测试边
  * 数据集中仅包含一个图
  * 训练时：用 a. 预测 b.
  * 验证时：用 a.、b.预测 c.
  * 测试时：用 a.、b.、c.预测 d

  <img src="./pics/op2.png" style="zoom:67%;" />



## Lecture 7. How Expressive are Graph Neural Networks

### 7.1 Introduction

* GNN 的表达性越强（即产生的嵌入向量越能区分不同图结构），效果就越好。

* GNN 产生的节点 $u$ 的嵌入向量由 $u$ 的计算图决定，计算图本质上是以 $u$ 为根的子树结构。
* 表达性强的 GNN 应该能把不同子树结构映射到不同的嵌入向量，即应尽可能（在每一层都做到）单射

因此，**表达性强的 GNN 应该使用单射的邻域聚合函数**。

<img src="./pics/subtree.png" style="zoom:80%;" />



### 7.2 Designing the Most Powerful Graph Neural Network

邻域聚合可以抽象为多重集合上的函数 （不同节点可能有相同的隐藏表示 $\mathbf{h}$）。

* GCN（平均池化）：取平均值后再使用线性函数和 $\text{ReLU}$ 激活函数
  $$
  Mean( \{x_u\}_{u∈N(v)})
  $$
  

  **定理**：无法区分各种隐藏表示比例相同的的不同多重集（如 $\{a, b\}$ 和 $\{a,a,b,b\}$）

  <img src="./pics/mean.png" style="zoom:67%;" />

* GraphSAGE（最大值池化）：应用一个 MLP，再对元素取最大值
  $$
  Max( \{x_u\}_{u∈N(v)})
  $$
  

  **定理**：无法区分隐藏表示种类构成相同的多重集（如 $\{a,b\}$ 和 $\{a,b,b\}$）

  <img src="./pics/max.png" style="zoom:67%;" />

如何设计一个表达性最强的 GNN？

* **定理**：任何单射多重集函数都可表示为 $\Phi\left(\sum_{x\in S}f(x)\right)$ ，其中 $\Phi$ 和 $f$ 都是非线性函数

  <img src="./pics/function.png" style="zoom:67%;" />

* **定理**（通用近似定理）

  > 含有 1 个隐藏层且隐藏层维度数充分大、激活函数合适（包括 ReLU 和 sigmoid）的 MLP 可以以任意精度近似任意连续函数

因此，我们可以用 MLP 来近似 $\Phi$ 和 $f$ 。
$$
\text{MLP}_{\Phi}\left(\sum_{x\in S}\text{MLP}_{f}(x)\right)
$$

> 实际应用中，MLP 隐藏层维度在100-500比较合适

这被称作图同构网络（Graph Isomorphism Network, GIN），GIN 的邻域聚合函数是单射的。

> GIN 是表达性最强的 GNN，可以视为 WL Graph Kernel 的发展
>
> WL Graph Kernel 的着色算法：$c^{(k+1)}(v)=\text{HASH}\left(c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u\in N(v)}\right)$，其中 $\text{HASH}$ 为任意哈希函数



**定理**：

以 $\left(c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u\in N(v)}\right)$ 为输入的任意单射函数都可以建模为
$$
\text{MLP}_{\Phi}\left ((1+\epsilon)\cdot \text{MLP}_{f}(c^{(k)}(v)))+\sum_{u\in N (v)}\text{MLP}_{f}(c^{(k)}(u))\right)
$$
其中 $\epsilon$ 是可学习参数

而如果最初的输入 $c^{(0)}(v)$ 是独热编码形式的，则直接相加就是单射的，不需要 $f$。

因此在 GIN 中
$$
\text{HASH}=\text{GINConv} \\
\text{GINConv}\left(c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u\in N(v)}\right)=\text{MLP}_{\Phi}\left((1+\epsilon)\cdot c^{(k)}(v)+\sum_{u\in N(v)}c^{(k)}(u)\right)
$$

即 GIN 的迭代方法是
$$
c^{(k+1)}(v)=\text{GlNConv}\left(\left\{c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u\in N(v)}\right\}\right)
$$
在进行 $K$ 步 GIN 迭代后，$c^{(K)}(v)$ 汇总了 $K$ 跳邻域的结构

相比于 WL Graph Kernel，GIN 的嵌入向量是低维的，粒度更细；而且更新函数的参数可以被学习并用于下游任务。



### 7.3 When Things Don't Go As Planned

* 数据预处理：对节点特征进行正则化
* 使用稳健的 ADAM 作为优化器
* 激活函数
  * 使用 ReLU 作为激活函数。LeakyReLU、PReLU 也不错。
  * 输出层不要放置激活函数
  * 每一层都要加入偏置项
* 嵌入向量维度数常用 32、64、128
* 仔细进行超参数选择、损失函数选择、权重参数初始化





## Lecture 8. Label Propagation on Graphs

<img src="./pics/explanation.png" style="zoom:80%;" />

### 8.1 How Do We Leverage Node Correlations in Networks?

**Problem**：一个图上部分节点有标签，如何以此来预测其它节点的标签

节点嵌入（通过随机游走、GNN ）可以做到，但效果不好。

一个替代方法是标签传播（label propagation）：它基于**节点通过边互相关联**的假设，也就是在图上靠得近的节点标签也接近。

> “ 物以类聚，人以群分 ”



### 8.2 Label Propagation

**标签传播的核心思想：节点 $v$ 的标签 $Y_v$ 为 $c$ 的概率 $P(Y_v=c)$ 是其相邻节点为该类概率的加权平均**

这里假设任务是做二分类（标签为 0 和 1），以红色表示标签为 0 的节点，以绿色表示标签为 1 的节点，以灰色表示标签未知的节点

<img src="./pics/problem.png" style="zoom:67%;" />

**标签传播（LP）算法**

1. 初始化

   * 简记 $P(Y_v=1)$ 为 $P{(Y_v)}$
   * 有标签的节点 $v$ ：若标签为 1 则 $P^{(0)}{(Y_v)}=1$，为 0 则 $P^{(0)}{(Y_v)}=0$
   * 无标签的节点 $v$：设置 $P^{(0)}_{Y_v}=0.5$ 

2. 迭代：更新每个无标签节点的标签
   $$
   P^{(t+1)}{(Y_v = c)}=\dfrac{1}{\sum_{(v,u)\in E}A_{v,u}}\sum_{(v,u)\in E}A_{v,u}P^{(t)}{(Y_u=c)}
   $$

   * 若图中的边有长度或权重信息，则 $A_{v, u}$ 为节点 $u, v$ 之间的边的权重
   * $P{(Y_v = c)}$ 表示节点 $v$ 具有标签 $c$ 的概率

   * 重复更新直至收敛，收敛判别准则：$\big|P^{(t)}{(Y_v)}-P^{(t-1)}{(Y_v)}\big|\leq\epsilon\quad\forall v\in V$

3. 输出

   * 最终的 $P{(Y_v)}>0.5$ 则标签为 1，$P{(Y_v)}<0.5$ 则为 0

缺点

* 收敛慢，甚至可能不收敛
* 未使用节点属性

### 8.3 Node Classification: Correct & Smooth

* LP：利用节点标签通过边相关连的假设，但不使用节点特征
* GNN：使用节点特征，但不利用节点标签相关的假设，即不同节点的预测结果独立/不相关

在节点特征非常具有预测性（predictive）时，GNN 效果很好。但当节点特征并不具有预测性时，GNN 会出错。例如有时两个对称的节点会有完全一样的计算树（图中 $v_2$ 和 $v_5$），但它们附近节点的标签不同，这个时候 LP 能很好区分两者但 GNN 完全不行。

![](./pics/10.png)



Correct & Smooth 算法（C&S） 

1. 训练基预测器（如简单的 MLP 或 GNN）；

2. 使用基预测器预测所有节点的 soft 标签（即节点 $v$ 属于各类的概率向量 $\hat{y}$）；

3. 通过图结构进行后处理（包括 1. 修正和 2. 平滑化）得到最终预测结果。

   

**修正**

* 思想：不同节点有不同程度的预测误差，**相连的节点预测误差大小也相近。**

* 初始化

  * 对于有标签的节点 $v$，计算其初始误差向量 $e_v^{(0)}=y_v-\hat{y}_v$，其中 $y_v$ 为真实标签的独热编码，$\hat{y}_v$ 为预测的概率向量
  * 对于没有标签的节点 $v$，$e_v^{(0)}=\mathbf{0}$
  * 将 $e_v^{(0)}$ 从上往下拼接，得到初始误差矩阵 $\mathbf{E}^{(0)}$

* 迭代

  * 对误差向量进行扩散
  * $\textbf{E}^{(t+1)}\leftarrow(1-\alpha)\cdot\textbf{E}^{(t)}+\alpha\cdot\widetilde{\textbf{A}}\textbf{E}^{(t)}$，其中 $\alpha$ 为超参数
  * $\widetilde{\textbf{A}}$ 为扩散矩阵，$\widetilde{A}=\textbf{D}^{-1/2}\textbf{A}\textbf{D}^{-1/2}$，而 $\boldsymbol{D}=\text{Diag}(d_1,\ldots,d_N)$ 为度数矩阵
  * 若 $i$ 与 $j$ 相连，则 $\widetilde{\textbf{A}}_{ij}=\frac{1}{\sqrt{d_i}\sqrt{d_j}}$，否则为 0
  * 因此如果 $i$ 与 $j$ 都之与彼此相连则 $\widetilde{\textbf{A}}$ 大，两者都还和很多别的节点相连则 $\widetilde{\textbf{A}}$ 小

* 输出

  * $\hat{y}_v \leftarrow s \cdot e_v^{(K)} + \hat{y}_v$，其中 $s$ 为超参数

    

**平滑化**

* 思想：相邻节点的标签倾向于相同

* 初始化

  * 对于有标签的节点 $v$，其初始标签向量 $z_v^{(0)}=y_v$
  * 对于没有标签的节点 $v$，$z_v^{(0)}=\hat{y}_v$
  * 将 $z_v^{(0)}$ 从上往下拼接，得到初始标签矩阵 $\mathbf{Z}^{(0)}$

* 迭代

  * $\textbf{Z}^{(t+1)}\leftarrow(1-\alpha)\cdot\textbf{Z}^{(t)}+\alpha\cdot\widetilde{\textbf{A}}\textbf{Z}^{(t)}$，其中 $\alpha$ 为超参数

* 输出

  * 平滑化所得结果 $z^{(K)}_v$ 各项之和不为 1，这时取最大值对应的位置作为最终预测

  

### 8.4 Masked Label Prediction

另一个显式地包括节点标签信息的办法（从 BERT 中获取灵感）

**掩盖标签预测**

* 思想：将标签视作特征
* 算法
  * 将标签矩阵 $Y$ 和 特征矩阵 $X$ 拼接
  * 记知道标签的节点的标签矩阵为 $\hat{Y}$
  * 随机将 $\hat{Y}$ 的一部分标签向量置为 $\mathbf{0}$（记掩盖标签），将转化后的结果记作 $\tilde{Y}$
  * 用 $[X,\tilde{Y}]$ 去预测被掩盖标签的节点的标签
  * 用 $\hat{Y}$ 去预测不知道标签的节点
* 与连接预测类似（自监督学习）





## Lecture 9. Machine Learning with Hetergeneous Graphs

### 9.1 Hetergeneous Graphs

异构图：有不同种类节点和不同种类的边的图。

异构图可以表示为 $G=(V,E,\tau,\phi)$

* 节点 $v\in V$ 的类型为 $\tau(v)$
* 边 $e=(u, v)\in E$ 的类型为 $\phi(u,v)$
* 边 $e=(u,v)$ 的关系（relation）$r(u,v)=(\tau(u),\phi(u,v),\tau(v))$

有时我们可以直接把节点的类型通过独热编码添加到 feature 里去从而将异构图转化为标准图（同构图）。但更多时候不行。因为不同类型的节点可能连特征的数量都不一样，而且不同的关系类型有不同的物理意义。



### 9.2 Relational GCN (RGCN)

同构图上的 GCN：$\quad\textbf{h}_v^{(l)}=\sigma\left(\textbf{W}^{(l)}\sum\limits_{u\in N(v)}\frac{\textbf{h}_u^{(l-1)}}{|N(v)|}\right)$

 **Problem**： 如何拓展 GCN 到异构图

对于不同的关系类型 $r_i$ 使用不同的神经网络权重 $W_{r_i}$。

* Relational GCN（RGCN） 
  $$\textbf{h}_v^{(l+1)}=\sigma\left(\sum_{r\in R}\sum_{u\in N_v^r}\dfrac{1}{c_{v,r}}\textbf{W}_r^{(l)}\textbf{h}_u^{(l)}+\textbf{W}_0^{(l)}\textbf{h}_v^{(l)}\right)$$
* 其中 $c_{v,r}=|N_v^r|$
* 消息函数
  * 对于关系为给定类型的相邻节点：$\mathbf{m}_{u,r}^{(l)}=\frac{1}{c_{v,r}}\mathbf{W}_{r}^{(l)}\mathbf{h}_{u}^{(l)}$
  * 自环：$\mathbf{m}_{u, r}^{(l)}=\mathbf{W}_0^{(l)}\mathbf{h}_v^{(l)}$
* 聚合函数
  * $\mathbf{h}_{v}^{(l+1)}=\sigma\left(\text{Sum}\left(\left\{\mathbf{m}_{u,r}^{(l)},u\in N(v)\right\}\cup\left\{\mathbf{m}_{v}^{(l)}\right\}\right)\right)$

参数量会随着关系类型数量增长而迅速增长！可能导致过拟合。如何压缩参数量？

* Solution 1：块对角矩阵（Block Diagonal Matrices）

  * 限制 $\mathbf{W}_r$ 为块对角矩阵
  * 若每个对角块的维度为 $B$，则 $\mathbf{W}_r$ 的大小从 $d^{(l+1)}\times d^{(l)}$ 降低到 $B\times\frac{d^{(l+1)}}{B}\times\frac{d^{(l)}}{B}$，其中 $d^{(l)}$ 为第 $l$ 层的隐藏表示的维度

* Solution 2：基学习（Basis Learning）

  * 在不同关系间共享权重
  * 将 $\mathbf{W}_r$ 表示为一系列基矩阵的线性组合，即 $\textbf{W}_r=\sum_{b=1}^B a_{rb}\cdot\textbf{V}_b$

* Example 1：节点分类

  * **这里的节点分类和节点 $v$ 的类型 $\tau(v)$ 不是一个东西！**
  * 如果共有 $k$ 类，直接让最后一层输出 $\textbf{h}^{(L)}\in\mathbb{R}^k$

* Example 2：连接预测

  * **注意：这里的图是有向图**

  * 参考连接预测，将边分成四份（训练消息、训练监督、验证和测试），每份中都有所有类型 $r$ 的边

  * RGCN 的最终输出： $\mathbf{h}^{(L)}_v \in \mathbb{R}^d$

  * 特定关系评分函数： $f_{r_i}:\mathbb{R}^d\times \mathbb{R}^d \rightarrow \mathbb{R}$，如 $f_{r_1}(\textbf{h}_E,\textbf{h}_A)=\textbf{h}_E^TW_{r_1}\textbf{h}_A,\textbf{W}_{r_1}\in\mathbb{R}^{d\times d}$

  * 训练

    1. 假设 $(E, r_3, A)$ 是训练监督边，其它是训练消息边
    2. 对训练消息边集用 RGCN 计算 $f_{r_3}(\mathbf{h}_E,\mathbf{h}_A)$
    3. 创建负边（负边不在训练监督边集和是训练消息边集中，**可以认为是不存在的边**），简单地说就是把 $(E,r_3,A)$ 中的目标节点 $A$ 换成别的和 $E$ 无 $r_3$ 关系的节点，这里假设负边有 $(E,r_3,B)$ 和 $(E,r_3,F)$
    4. 计算负边的评分，即 $f_{r_3}(\mathbf{h}_E,\mathbf{h}_B)$ 和 $f_{r_3}(\mathbf{h}_E,\mathbf{h}_F)$
    5. 将交叉熵作为损失函数，从而最大化训练监督边的评分，最小化负边的评分。
       *  这里 $\ell=\text{CE}((1,0,0),(f_{r_3}(\mathbf{h}_E,\mathbf{h}_A),f_{r_3}(\mathbf{h}_E,\mathbf{h}_B),f_{r_3}(\mathbf{h}_E,\mathbf{h}_F)))$
       *  若只有一个负边 $(E,r_3,B)$：$\ell=-\log\sigma\left (f_{r_3}(\mathbf{h}_E,\mathbf{h}_A)\right)-\log \big(1-\sigma\big (f_{r_3}(\mathbf{h}_E,\mathbf{h}_B)\big)\big)$ 

  * 验证

    1. 假设 $(E,r_3,D)$ 为验证边
    2. 对训练消息边集+训练监督边集用 RGCN 计算 $f_{r_3}(\mathbf{h}_E,\mathbf{h}_D)$
    3. 创建负边并计算其评分，这里是 $f_{r_3}(\mathbf{h}_E,\mathbf{h}_B)$ 和 $f_{r_3}(\mathbf{h}_E,\mathbf{h}_F)$
    4. 将以上所有评分排序，记 $\text{RK}$ 为 $(E,r_3,D)$ 的排名
    5. 计算 metrics
       1. $\mathbf{1}[\text{RK} \le k]$：越大越好
       2. $\frac{1}{\text{RK}}$：越大越好

    

### 9.3 Heterogeneous Graph Transformer

同构图上的 GAT：$\textbf{h}_v^{(l)}=\sigma(\sum_{u\in N(v)}{\alpha_{vu}}\textbf{W}^{(l)}\textbf{h}_u^{(l-1)})$ ，

如何扩展 GAT 到异构图？

异构图 Transformer（HGT）使用 [缩放点积注意力](https://zhuanlan.zhihu.com/p/353680367)。
$$
\text{Attention}(Q,K,V)=\text{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V
$$
其中 $Q$ 为查询，$K$ 为键，$V$ 为值，三者形状都为 `(batch_size, dim)`。通过线性层可以得到 $Q,K,V$：$T=\text{T-Linear} (X), \quad T\in \{Q, K, V\}$。

我们希望注意力能考虑关系类型 $r$ 这一信息，但又不希望参数量过大：将异构图注意力分解为节点类型相关和边类型相关。

考虑节点 $s,t$ 和边 $e$ 构成的关系 $r(u,v)=(\tau(u),\phi(u,v),\tau(v))$。

$$
\text{ATT-head}^i(s,e,t)=\left(K^i(s)W_{\phi(e)}^{\text{ATT}}Q^i(t)^T\right)
$$

$$
K^i(s)=\operatorname{K-Linear}^i_{\tau(s)}\left(H^{(l-1)}[s]\right)
$$

$$
Q^i(t)=Q\text{-Linear}^i_{\tau(t)}\Big(H^{(l-1)}[t]\Big)
$$

这里我们看到 $\text{K-Linear}$ 只由注意力头编号 $i$ 和节点 $s$ 的类型 $\tau(s)$ 决定，$\text{Q-Linear}$ 也类似。$W_{\phi(e)}^{\text{ATT}}$ 则只由边的类型 $\phi (e)$ 决定。因此注意力机制被分解了。

而整体上
$$
\widetilde{H}^{(l)}[t]=\underset{\forall s\in N(t)}{\bigoplus}\biggr({\textbf{Attention}_{\text{HGT}}(s,e,t)}\cdot\textbf{Message}_{\text{HGT}}(s,e,t)\biggr)
$$
Attention 部分已经求出，现在来看 Message 部分。

与 Attention 一样，HGT 将 Message 部分也分解到节点类型相关和边类型相关。

$$
\textbf{Message}_{\text{HGT}}(s,e,t)=\underset{i\in[1,h]}{||}\text{MSG-head}^i(s,e,t)
$$

$$
\text{MSG-head}^i(s,e,t)=\text{M-Linear}^i_{\tau(s)}(H^{(l-1)}[s])W^{\text{MSG}}_{\phi(e)}
$$



### 9.4 Design Space of Heterogeneous GNNs

如何将同构图的 GNN 设计空间扩展到异构图?

* 消息计算
  * 同构图
    * $\textbf{m}_u^{(l)}=\text{MSG}^{(l)}\left(\textbf{h}_u^{(l-1)}\right)$
    * 具体实现：$\textbf{m}_u^{(l)}=\textbf{W}^{(l)}\textbf{h}_u^{(l-1)}$
  * 异构图
    * 不同的关系类型应该有不同的消息函数
    * $\mathbf{m}_{u}^{(l)}=\text{MSG}_{r}^{(l)}\left(\mathbf{h}_{u}^{(l-1)}\right),r=(u,e,v)$
    * 具体实现：$\textbf{m}_u^{(l)}=\textbf{W}_r^{(l)}\textbf{h}_u^{(l-1)}\quad$
* 聚合计算
  * 同构图
    * $\mathbf{h}_v^{(l)}=\text{AGG}^{(l)}\left(\left\{\mathbf{m}_u^{(l)},u\in N(v)\right\}\right)$
    * 具体实现：$\textbf{h}_v^{(l)}=\text{Sum}(\{\textbf{m}_u^{(l)},u\in N(v)\})$
  * 异构图
    * 分两步走：先在同一关系类型内部聚合，再聚合不同关系类型
    * $\mathbf{h}_{v}^{(l)}=\mathrm{AG}_{alll}^{(l)}\left(\mathrm{AGG}_{r}^{(l)}\left(\left\{\mathbf{m}_{u}^{(l)},u\in N_{r}(v)\right\}\right)\right)$
    * 具体实现：$\mathbf{h}_{v}^{(l)}=\text{Concat}\Big(\text{Sum}\Big(\big\{\mathbf{m}_{u}^{(l)},u\in N_{r}(v)\big\}\Big)\Big)$
* 层连通性
  * 同构图
    * 跳过连接、预/后处理
  * 异构图
    * 跳过连接和同构图一样
    * 预/后处理要考虑节点类型：$\textbf{h}_v^{(l)}=\text{MLP}_{\tau(v)}(\textbf{h}_v^{(l)})$
* 图操作
  * 同构图
    * 特征增强、添加虚拟节点/边、邻域取样、子图取样
  * 异构图
    * 根据关系类型进行上述操作
* GNN 预测头
  * 同构图
    * 节点级预测：$\widehat{\mathbf{y}}_v=\text{Head}_{\text{node}}(\mathbf{h}_v^{(L)})=\mathbf{W}^{(H)}\mathbf{h}_v^{(L)}$
    * 边级预测：$\widehat{\mathbf{y}}_{uv}=\text{Head}_{\text{edge}} (\mathbf{h}_{u}^{(L)},\mathbf{h}_{v}^{(L)})=\text{Linear}(\text{Contat}(\mathbf{h}_{u}^{(L)},\mathbf{h}_{v}^{(L)}))$
    * 图级预测：$\widehat{\mathbf{y}}_G=\text{Head}_{\text{graph}}(\{\mathbf{h}_v^{(L)}\in\mathbb{R}^d,\forall v\in G\})$
  * 异构图
    *  节点级预测：$\widehat{\mathbf{y}}_v=\text{Head}_{\text{node},\tau(v)}(\mathbf{h}_v^{(L)})=\mathbf{W}_{\tau(v)}^{(H)}\mathbf{h}_v^{(L)}$
    *  边级预测：$\widehat{\mathbf{y}}_{uv}=\text{Head}_{\text{edge},r} (\mathbf{h}_{u}^{(L)},\mathbf{h}_{v}^{(L)})=\text{Linear}_r(\text{Contat}(\mathbf{h}_{u}^{(L)},\mathbf{h}_{v}^{(L)}))$
    *  图级预测：$\widehat{\mathbf{y}}_G=\text{AGG}(\text{Head}_{\text{graph},i}(\{\mathbf{h}_v^{(L)}\in\mathbb{R}^d,\forall \tau(v)=i\}))$

**本质上就是把不同的关系类型分开来建模。**



## Lecture 10. Knowledge Graph Embeddings

### 10.1 Knowledge Graph Completion

知识图谱（KG）是一种异构图：

* 节点称为实体
* 边称为关系

* 知识图谱的特征是数据量大但是容易不完整

**Problem：**如何补全？即给定 (head, relation) 如何预测 tail？

KG 中的边表示为三元组 $(h,r,t)$，其中 $h$ 为头节点，$r$ 为关系，$t$ 为尾节点。

我们给每个实体和关系直接通过 embedding-lookup 方法指定一个嵌入向量，这被称作 shallow embedding。我们的目标是 $(h,r)$ 的嵌入向量能尽可能接近 $t$ 的嵌入向量。**注意，这里我们不使用 GNN。**

我们首先讨论 KG 可能具有的一些关系模式

* 对称性：$r (h, t)\Rightarrow r(t,h)\quad\forall t,h$
* 反对称性：$r (h, t)\Rightarrow \neg r(t,h)\quad\forall t,h$
* 逆关系：$r_1 (h, t)\Rightarrow r_2(t,h)\quad\forall t,h$
* 复合（传递）关系：$r_1(x,y)\wedge r_2(y,z)\Rightarrow r_3(x,z)\quad\forall x,y,z$
* 一对多关系：$r (h, t_1),r (h, t_2),...,r (h, t_n)$ 同时成立
  （注意这些关系里的 $\forall t,h$）

KG 可能含有以上的部分或全部关系模式，合适的 KG embedding 模型应当能够反映这些关系模式。

评分函数 $f_r(h,t)$ 描述 $h,t$ 之间有关系 $r$ 的可能性（不等于概率），值越大越好。不同的 KG embedding 模型定义不同的评分函数。



### 10.2 KG embedding 模型

* **Knowledge Graph Completion: TransE**

  > 最为符合直觉，直接把三者都嵌入到 $d$ 维空间，即 $\textbf{h},\textbf{r},\textbf{t}\in\mathbb{R}^d$
  >
  > 以空间中的距离作为评分函数 $f_r(h,t)=-\|\textbf{h}+\textbf{r}-\textbf{t}\|$
  >
  > 缺点：无法反映对称性和一对多关系。

* **Knowledge Graph Completion: TransR**

  > TransE 基础上的提升，先嵌入到 $k$ 维空间，再通过矩阵 $\mathbf{M}_r$ 映射到 $d$ 维空间，即 $\textbf{h,t,r}\in\mathbb{R}^k$，$\mathbf{M}_r\in\mathbb{R}^{d\times k}$
  >
  > 评分函数是 $f_r(h,t)=-\|\boldsymbol{M}_r\textbf{h}+\textbf{r} -\boldsymbol{M}_r\textbf{t}\|$
  >
  > 特点：解决了无法反映对称性和一对多关系的问题，可以反映全部的关系模式。



* **Knowledge Graph Completion: DistMult**

  > 使用一种双线性的模型
  >
  > 评分函数是 $f_r(h,t)=<\textbf{h},\textbf{r},\textbf{t}>=\sum_i\textbf{h}_i\cdot\textbf{r}_i\cdot\textbf{t}_i$，其中 $\textbf{h},\textbf{r},\textbf{t}\in\mathbb{R}^k$
  >
  > **可以视作 $\mathbf{h}*\mathbf{r}$ （指 MATLAB 中的向量按位相乘）和 $\mathbf{t}$ 的 cos 相似度**
  >
  > 缺点：无法反映反对称、逆关系和和传递关系。

* **Knowledge Graph Completion: ComplEx**

  > DistMult 基础上的提升，将实体与关系 embed 到复向量空间，即 $\textbf{h},\textbf{r},\textbf{t}\in\mathbb{C}^k$
  >
  > 评分函数为 $f_r(h,t)=\text{Re}(\sum_i\mathbf{h}_i\cdot\mathbf{r}_i\cdot\bar{\mathbf{t}}_i)$
  >
  > 缺点：无法反映传递关系

**Conclusion**

<img src="./pics/conclusion.png" style="zoom:120%;" />

* 结合表格选取合适的模型
* 若 KG 没有大量对称关系，可以很快地试着跑一下 TransE
* 随后再使用复杂模型，如ComplEx



